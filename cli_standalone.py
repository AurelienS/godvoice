#!/usr/bin/env python3
"""
GodVoice CLI - Analyseur de conversations WhatsApp (Version autonome)
Usage: godvoice --chat-file chat.txt --anthropic-key sk-ant-...
"""

import click
import os
import sys
import re
from collections import defaultdict
from pathlib import Path
import pandas as pd

# Configuration matplotlib pour l'ex√©cutable
import matplotlib
matplotlib.use('Agg')  # Backend non-interactif pour l'ex√©cutable
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import tiktoken
import seaborn as sns

# === Utilitaires ===
def count_tokens(text):
    """Compte les tokens dans un texte"""
    try:
        enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))
    except Exception as e:
        # Fallback: estimation approximative bas√©e sur les mots
        # Approximation: 1 token ‚âà 0.75 mots en fran√ßais/anglais
        word_count = len(text.split())
        estimated_tokens = int(word_count / 0.75)
        if hasattr(click, 'echo'):
            click.echo(f"‚ö†Ô∏è  Tiktoken non disponible, estimation approximative: {estimated_tokens} tokens")
        return estimated_tokens

def extract_hour(t):
    """Extrait l'heure d'un timestamp"""
    try:
        return int(str(t).split(':')[0])
    except Exception:
        return None

def parse_whatsapp_chat(chat_file):
    """Parse le fichier de chat WhatsApp"""
    click.echo(f"üì± Parsing du fichier: {chat_file}")

    # Regex pour d√©tecter d√©but de message WhatsApp
    pattern = re.compile(r'^(\d{1,2}/\d{1,2}/\d{2,4}), (\d{1,2}:\d{2}) - ([^:]+): (.+)')

    # Stockage des messages par auteur
    user_messages = defaultdict(list)
    # Stockage sous forme de liste pour pandas
    all_messages = []

    with open(chat_file, "r", encoding="utf-8") as f:
        current_author = None
        current_date = None
        current_time = None
        for line in f:
            line = line.strip()
            if not line:
                continue

            match = pattern.match(line)
            if match:
                date, time, author, message = match.groups()
                current_author = author
                current_date = date
                current_time = time
                user_messages[author].append(message)
                all_messages.append({
                    'date': date,
                    'time': time,
                    'author': author,
                    'message': message
                })
            elif current_author:
                # Ajoute la ligne au dernier message
                user_messages[current_author][-1] += " " + line
                # Ajoute aussi √† la liste pour pandas
                if all_messages:
                    all_messages[-1]['message'] += " " + line

    click.echo(f"‚úÖ {len(all_messages)} messages pars√©s de {len(user_messages)} auteurs")
    return user_messages, all_messages

def create_chunks(user_messages, output_dir="by_authors", chunk_token_limit=750):
    """Cr√©e les chunks par auteur"""
    click.echo(f"üìÅ Cr√©ation des chunks dans {output_dir}/")

    Path(output_dir).mkdir(exist_ok=True)

    for author, messages in user_messages.items():
        author_dir = Path(output_dir) / author.replace(" ", "_")
        author_dir.mkdir(parents=True, exist_ok=True)

        # D√©coupage intelligent par tokens
        chunks = []
        current_chunk = []
        current_tokens = 0
        for msg in messages:
            msg_tokens = count_tokens(msg)
            if current_tokens + msg_tokens > chunk_token_limit and current_chunk:
                chunks.append("\n".join(current_chunk))
                current_chunk = []
                current_tokens = 0
            current_chunk.append(msg)
            current_tokens += msg_tokens
        if current_chunk:
            chunks.append("\n".join(current_chunk))

        for idx, chunk in enumerate(chunks, 1):
            filename = author_dir / f"chunk_{idx:03}.txt"
            with open(filename, "w", encoding="utf-8") as out:
                out.write(chunk)

    click.echo("‚úÖ Chunks g√©n√©r√©s")

def generate_statistics(all_messages, figures_dir="figures", top_n=10):
    """G√©n√®re les statistiques et graphiques"""
    click.echo(f"üìä G√©n√©ration des statistiques (top {top_n})")

    # Cr√©ation du DataFrame
    df = pd.DataFrame(all_messages)
    df['nb_mots'] = df['message'].apply(lambda x: len(x.split()))
    df['hour'] = df['time'].apply(extract_hour)

    # Statistiques par auteur
    stats = df.groupby('author').agg(
        nb_messages=('message', 'count'),
        nb_mots=('nb_mots', 'sum'),
        moyenne_mots=('nb_mots', 'mean')
    )

    click.echo("\n=== Statistiques par auteur ===")
    click.echo(stats.sort_values('nb_messages', ascending=False).head(top_n).to_string())

    # Cr√©ation du dossier figures
    os.makedirs(figures_dir, exist_ok=True)

    # Top auteurs
    top_authors = stats['nb_messages'].sort_values(ascending=False).head(top_n)

    try:
        # Graphique du nombre de messages par auteur
        plt.figure(figsize=(10, 6))
        top_authors.plot(kind='bar')
        plt.title(f'Top {top_n} - Nombre de messages par auteur')
        plt.xlabel('Auteur')
        plt.ylabel('Nombre de messages')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(os.path.join(figures_dir, 'messages_par_auteur_top.png'))
        plt.close()

        # Participation dans le temps
        df['date'] = pd.to_datetime(df['date'], format='%m/%d/%y', errors='coerce')
        daily_counts = df.groupby(['date', 'author']).size().unstack(fill_value=0)
        top_authors_list = top_authors.index.tolist()
        daily_counts_top = daily_counts[top_authors_list]
        plt.figure(figsize=(14, 6))
        daily_counts_top.plot(ax=plt.gca())
        plt.title(f'Top {top_n} - Participation quotidienne par auteur')
        plt.xlabel('Date')
        plt.ylabel('Nombre de messages')
        plt.legend(title='Auteur', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(os.path.join(figures_dir, 'participation_quotidienne_top.png'))
        plt.close()

        # R√©partition horaire
        hourly_df = df.dropna(subset=['hour'])
        hourly_counts = hourly_df.groupby(['hour', 'author']).size().unstack(fill_value=0)
        hourly_counts_top = hourly_counts[top_authors_list]
        plt.figure(figsize=(14, 6))
        hourly_counts_top.plot(ax=plt.gca())
        plt.title(f'Top {top_n} - R√©partition horaire des messages par auteur')
        plt.xlabel('Heure de la journ√©e')
        plt.ylabel('Nombre de messages')
        plt.legend(title='Auteur', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig(os.path.join(figures_dir, 'repartition_horaire_top.png'))
        plt.close()

        # Pourcentage de participation (camembert)
        plt.figure(figsize=(8, 8))
        top_authors.plot(kind='pie', autopct='%1.1f%%', startangle=90, colormap='tab20')
        plt.title(f'Top {top_n} - Pourcentage de participation par auteur')
        plt.ylabel('')
        plt.tight_layout()
        plt.savefig(os.path.join(figures_dir, 'pourcentage_participation_top.png'))
        plt.close()

        click.echo("‚úÖ Graphiques g√©n√©r√©s")
    except Exception as e:
        click.echo(f"‚ö†Ô∏è  Erreur lors de la g√©n√©ration des graphiques: {e}")
        click.echo("üìä Statistiques g√©n√©r√©es sans graphiques")

    return df, stats, top_authors

def run_ai_analysis(user_messages, top_authors, anthropic_key, figures_dir="figures", output_dir="by_authors"):
    """Lance l'analyse IA avec Anthropic"""
    click.echo("ü§ñ D√©marrage de l'analyse IA...")

    try:
        from anthropic import Anthropic
    except ImportError:
        click.echo("‚ùå Librairie Anthropic non disponible")
        return

    if not anthropic_key:
        click.echo("‚ùå Cl√© Anthropic manquante")
        return

    anthropic_client = Anthropic(api_key=anthropic_key)
    click.echo("‚úÖ Anthropic configur√© (Claude Sonnet 3.5)")

    # Cr√©er le dossier de r√©sultats IA
    ai_results_dir = os.path.join(figures_dir, 'analyses_ia')
    os.makedirs(ai_results_dir, exist_ok=True)

    def analyze_with_anthropic(text, author, analysis_type="general"):
        prompts = {
            "sentiment": f"Analyse le sentiment de {author}. Score 1-10 (1=n√©gatif, 10=positif) + analyse d√©taill√©e des √©motions dominantes et nuances psychologiques.",
            "topics": f"Top 5 sujets principaux de {author}. Pour chaque sujet: titre, fr√©quence, contexte, et exemples de messages repr√©sentatifs.",
            "style": f"Style de communication de {author}: personnalit√© d√©taill√©e, registre de langue, tics linguistiques, expressions favorites, √©volution dans le temps.",
            "summary": f"Portrait complet de {author}: personnalit√©, r√¥le dans le groupe, relations avec les autres, √©volution, anecdotes marquantes.",
            "political": f"üå∂Ô∏è ANALYSE POLITIQUE APPROFONDIE de {author}: Orientation politique (gauche/droite/centre), positions sur immigration, √©conomie, √©cologie, soci√©t√©. Cite des messages pr√©cis r√©v√©lateurs de ses convictions.",
            "controversial": f"üî• CONTENU CLIVANT ET POL√âMIQUE de {author}: D√©bats houleux, prises de position controvers√©es, conflits, sujets sensibles. Analyse le niveau de provocation et cite les messages les plus √©pic√©s.",
            "humor": f"üòÇ ANALYSE HUMOUR COMPL√àTE de {author}: Type d'humour (noir, absurde, sarcastique, potache), niveau de politiquement incorrect, meilleures vannes, r√©actions du groupe √† ses blagues.",
            "secrets": f"üïµÔ∏è R√âV√âLATIONS ET RAGOTS de {author}: Confessions personnelles, secrets r√©v√©l√©s, anecdotes croustillantes, indiscretions, vie priv√©e d√©voil√©e dans le groupe."
        }

        try:
            response = anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=800,
                messages=[
                    {"role": "user", "content": f"{prompts.get(analysis_type, prompts['summary'])}\n\nMessages de {author}:\n{text[:15000]}"}
                ]
            )
            return response.content[0].text
        except Exception as e:
            click.echo(f"Erreur Anthropic pour {author}: {e}")
            return None

    # Analyser le TOP des auteurs
    for author in top_authors.head(10).index:
        click.echo(f"üîç Analyse de {author}...")

        # Lire les chunks de cet auteur
        author_dir = Path(output_dir) / author.replace(" ", "_")
        if not author_dir.exists():
            continue

        # Combiner TOUS les chunks de cet auteur
        author_text = ""
        chunk_files = sorted(author_dir.glob("chunk_*.txt"))
        for chunk_file in chunk_files:
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunk_content = f.read()
                author_text += chunk_content + "\n"

        if len(author_text.strip()) == 0:
            continue

        click.echo(f"  üìù {len(author_text)} caract√®res analys√©s pour {author}")

        # Analyses COMPL√àTES
        analyses = {}
        analysis_types = ["sentiment", "topics", "style", "summary", "political", "controversial", "humor", "secrets"]

        for analysis_type in analysis_types:
            click.echo(f"  üîç {analysis_type}...")
            result = analyze_with_anthropic(author_text, author, analysis_type)
            if result:
                analyses[analysis_type] = result

        # Sauvegarder les r√©sultats
        if analyses:
            safe_author = author.replace(' ', '_').replace('.', '_')
            filename = f"{safe_author}_PREMIUM_analysis.txt"

            with open(os.path.join(ai_results_dir, filename), 'w', encoding='utf-8') as f:
                f.write(f"üèÜ === ANALYSE PREMIUM de {author} ===\n\n")
                f.write(f"üìä Donn√©es analys√©es: {len(author_text):,} caract√®res\n")
                f.write(f"üìÅ Chunks trait√©s: {len(chunk_files)}\n")
                f.write(f"ü§ñ Mod√®le utilis√©: Claude Sonnet 3.5\n\n")

                for analysis_type, result in analyses.items():
                    emoji_map = {
                        "sentiment": "üòä", "topics": "üìã", "style": "‚úçÔ∏è", "summary": "üìù",
                        "political": "üèõÔ∏è", "controversial": "üî•", "humor": "üòÇ", "secrets": "üïµÔ∏è"
                    }
                    emoji = emoji_map.get(analysis_type, "üìå")
                    f.write(f"{emoji} ## {analysis_type.upper()}\n{result}\n\n")

    click.echo("‚úÖ Analyse IA termin√©e ! R√©sultats dans figures/analyses_ia/")

@click.command()
@click.option('--chat-file', '-c',
              required=True,
              type=click.Path(exists=True, readable=True),
              help='Chemin vers le fichier chat.txt √† analyser')
@click.option('--anthropic-key', '-k',
              help='Cl√© API Anthropic (sk-ant-...)')
@click.option('--output-dir', '-o',
              default='by_authors',
              help='Dossier de sortie pour les chunks (d√©faut: by_authors)')
@click.option('--figures-dir', '-f',
              default='figures',
              help='Dossier de sortie pour les graphiques (d√©faut: figures)')
@click.option('--chunk-tokens',
              default=750,
              type=int,
              help='Limite de tokens par chunk (d√©faut: 750)')
@click.option('--top', '-t',
              default=10,
              type=int,
              help='Nombre d\'auteurs √† afficher dans les graphiques (d√©faut: 10)')
@click.option('--no-ai',
              is_flag=True,
              help='D√©sactiver l\'analyse IA (seulement statistiques)')
@click.option('--verbose', '-v',
              is_flag=True,
              help='Mode verbeux')
def main(chat_file, anthropic_key, output_dir, figures_dir, chunk_tokens, top, no_ai, verbose):
    """
    üé§ GodVoice - Analyseur de conversations WhatsApp avec IA

    Analyse les conversations WhatsApp, g√©n√®re des statistiques et des analyses IA.
    """

    if verbose:
        click.echo("üé§ GodVoice CLI - D√©marrage de l'analyse...")
        click.echo(f"üìÅ Fichier chat: {chat_file}")
        click.echo(f"ü§ñ API Anthropic: {'‚úÖ Configur√©e' if anthropic_key else '‚ùå Manquante'}")

    # V√©rifications
    if not no_ai and not anthropic_key:
        click.echo("‚ö†Ô∏è  Cl√© Anthropic non fournie. Analyse IA d√©sactiv√©e.")
        click.echo("   Utilisez --anthropic-key ou --no-ai pour supprimer cet avertissement.")
        no_ai = True

    click.echo("üöÄ GodVoice CLI - Analyseur de conversations WhatsApp")
    click.echo(f"üì± Fichier de chat: {chat_file}")
    click.echo(f"üìÅ Dossier de sortie: {output_dir}")
    click.echo(f"üìä Top auteurs: {top}")
    click.echo(f"ü§ñ Analyse IA: {'Activ√©e' if not no_ai else 'D√©sactiv√©e'}")
    click.echo()

    try:
        # √âtape 1: Parse le chat
        user_messages, all_messages = parse_whatsapp_chat(chat_file)

        # √âtape 2: Cr√©e les chunks
        create_chunks(user_messages, output_dir, chunk_tokens)

        # √âtape 3: G√©n√®re les statistiques
        df, stats, top_authors = generate_statistics(all_messages, figures_dir, top)

        # √âtape 4: Analyse IA (si activ√©e)
        if not no_ai and anthropic_key:
            run_ai_analysis(user_messages, top_authors, anthropic_key, figures_dir, output_dir)

        click.echo("\nüéâ Analyse termin√©e avec succ√®s !")
        click.echo(f"üìÅ Chunks: {output_dir}/")
        click.echo(f"üìä Graphiques: {figures_dir}/")
        if not no_ai and anthropic_key:
            click.echo(f"ü§ñ Analyses IA: {figures_dir}/analyses_ia/")

    except KeyboardInterrupt:
        click.echo("\n‚ö†Ô∏è  Analyse interrompue par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        click.echo(f"\n‚ùå Erreur: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()